import torch

t_u = [35.7, 55.9, 58.2, 81.9, 56.3, 48.9, 33.9, 21.8, 48.4, 60.4, 68.4]
t_c = [0.5, 14.0, 15.0, 28.0, 11.0, 8.0, 3.0, -4.0, 6.0, 13.0, 21.0]

t_u = torch.tensor(t_u) #input
t_c = torch.tensor(t_c) #ground truth

def model(t_u, w, b):
    return w * t_u + b

t_p = model(t_u, 1.0, 1.0)

# mean square loss function
def loss_fn(t_p, t_c):
    return ((t_p - t_c)**2).mean()

# hyper-parameters
delta = 0.1
learning_rate = 0.01
w = 1.0
b = 0.0

# delta L / delta w
loss_rate_change_wrt_w = (loss_fn(model(t_u, w + delta, b), t_c) - loss_fn(model(t_u, w - delta, b), t_c)) / 2.0 * delta
# delta L / delta b
loss_rate_change_wrt_b = (loss_fn(model(t_u, w, b + delta), t_c) - loss_fn(model(t_u, w, b - delta), t_c)) / 2.0 * delta

w = w - learning_rate * loss_rate_change_wrt_w
b = b - learning_rate * loss_rate_change_wrt_b

w,b


# d_l / d_m
def d_loss_wrt_model(t_p, t_c):
    return 2.0 * (t_p - t_c) / t_p.size(0)

# d_m / d_w
def d_model_wrt_w(t_u, w, b):
    return t_u
    
# d_m / d_b
def d_model_wrt_b(t_u, w, b):
    return 1.0
    
# gradient of loss wrt params
def gradient_fn(t_u, t_c, t_p, w, b):
    d_loss_wrt_w = d_loss_wrt_model(t_p, t_c) * d_model_wrt_w(t_u, w, b)
    d_loss_wrt_b = d_loss_wrt_model(t_p, t_c) * d_model_wrt_b(t_u, w, b)
    return torch.stack([d_loss_wrt_w.sum(), d_loss_wrt_b.sum()])

gradient_fn(t_u, t_c, t_p, w, b)


def training_loop(n_epoch, learning_rate, params, t_c, t_u):
    for epoch in range(1, n_epoch + 1):
        w, b = params
        t_p = model(t_u, w, b)
        grad = gradient_fn(t_u, t_c, t_p, w, b)
        params = params - learning_rate * grad # gradient descent
        if epoch % 1000 == 0:
            print("epoch %d loss %f" %(epoch, loss_fn(t_p, t_c)))
    return params

training_loop(n_epoch = 10000,
              learning_rate = 0.0001,
              params = torch.tensor([1.0, 0.0]),
              t_c = t_c,
              t_u = t_u)


t_u_normalized = t_u * 0.1

params = training_loop(n_epoch = 10000,
              learning_rate = 0.01,
              params = torch.tensor([1.0, 0.0]),
              t_c = t_c,
              t_u = t_u_normalized)


params


from matplotlib import pyplot as plt

t_p = model(t_u_normalized, *params)
plt.figure(dpi = 100)
plt.xlabel("measurement")
plt.ylabel("celcius")
plt.plot(t_u.numpy(), t_p.numpy())
plt.plot(t_u.numpy(), t_c.numpy(), 'o')


params = torch.tensor([1.0, 0.0], requires_grad=True)

loss = loss_fn(model(t_u_normalized, *params), t_c)
if params.grad is not None:
        params.grad.zero_()
loss.backward() #forcing back propogation
params.grad


def training_loop_2(n_epochs, learning_rate, params, t_c, t_u):
    for epoch in range(1, 1 + n_epochs):
        t_p = model(t_u, *params)
        loss = loss_fn(t_p, t_c)
        if params.grad is not None:
            params.grad.zero_()
        loss.backward() # gradient is computed and stored in params.grad
        with torch.no_grad():
            params -= learning_rate * params.grad # gradient descent
        if (epoch % 1000 == 0):
            print("epoch %d loss %f" %(epoch, loss)) 
    return params


params = training_loop_2(n_epochs = 10000,
              learning_rate = 0.01,
              params = torch.tensor([1.0, 0.0], requires_grad=True),
              t_c = t_c,
              t_u = t_u_normalized)


import torch.optim as optim

params = torch.tensor([1.0, 0.0], requires_grad=True)
learning_rate = 0.001
optimizer = optim.SGD([params], lr=learning_rate)
t_p = model(t_u, *params)
loss = loss_fn(t_p, t_c)
optimizer.zero_grad()
loss.backward()
optimizer.step()



