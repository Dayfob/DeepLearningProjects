{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "03cb0579-9bcf-4f0e-b222-05c78ef37ff0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n",
      "50000 10000\n"
     ]
    }
   ],
   "source": [
    "# Download CIFAR 10 dataset for training and validation purposes and apply the following changes on each image:\n",
    "# 1) make it a tensor\n",
    "# 2) normalize it based on the mean and standard deviation among all pixels in each channel (RGB).\n",
    "# Print the size of training and validation datasets\n",
    "\n",
    "from torchvision import datasets\n",
    "from torchvision import transforms\n",
    "import torch\n",
    "\n",
    "# default dataset needed for normalization:\n",
    "dataset_train = datasets.CIFAR10('.', train=True, download=True, transform=transforms.ToTensor())\n",
    "\n",
    "# calculation values for normalization:\n",
    "imgs = torch.stack([img for (img, label) in dataset_train], dim = 3)\n",
    "mean_deviation = imgs.view(3, -1).mean(dim=1)\n",
    "standard_deviation = imgs.view(3, -1).std(dim=1)\n",
    "\n",
    "# normalized datasets:\n",
    "dataset_train_trans = datasets.CIFAR10('.', train=True, download=True, transform=transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean_deviation.numpy(), standard_deviation.numpy())\n",
    "]))\n",
    "dataset_val_trans = datasets.CIFAR10('.', train=False, download=True, transform=transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean_deviation.numpy(), standard_deviation.numpy())\n",
    "]))\n",
    "\n",
    "# print lengths:\n",
    "len_dataset_train_trans = len(dataset_train_trans)\n",
    "len_dataset_val_trans = len(dataset_val_trans)\n",
    "\n",
    "print(len_dataset_train_trans, len_dataset_val_trans)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "90374899-4072-4ee4-a3c8-a1b0ed19ce13",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "15000 3000\n"
     ]
    }
   ],
   "source": [
    "# We want to make a tertiary classifier that distinguishes between deers, dogs, and horses, labeled as 4, 5, and 7, resp.\n",
    "# Create the subset training and validation datasets for this purpose.\n",
    "# Print the size of these datasets.\n",
    "\n",
    "label_map = {4:0, 5:1, 7:2}\n",
    "class_names = [\"deers\", \"dogs\", \"horses\"]\n",
    "\n",
    "dataset_train_filtered  =   [(img, label_map[label]) for img, label in dataset_train_trans if label in [4,5,7]]\n",
    "dataset_val_filtered    =   [(img, label_map[label]) for img, label in dataset_val_trans if label in [4,5,7]]\n",
    "\n",
    "# print lengths:\n",
    "len_dataset_train = len(dataset_train_filtered)\n",
    "len_dataset_val = len(dataset_val_filtered)\n",
    "\n",
    "print(len_dataset_train, len_dataset_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "5e45ff49-791b-4e63-9d35-35411a0b8abb",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Create a parameterized CNN with the following details. \n",
    "# The parameter is the number of output channels n after the first convolution.\n",
    "# All kernels are of size 3 by 3.\n",
    "# Convolutions must not change the height and width.\n",
    "# Each convolution is followed by hyperbolic tangent as the activation function, and max pooling of size 2 by 2.\n",
    "# Convolution ayers:\n",
    "# 1) First convolution layer works on the input RGB input. Let's assume there are n kernels in this layer.\n",
    "# 2) Second convolution layer works on the result of the preceding max pooling layer. \n",
    "#    Let's assume there are n/2 kernels in this layer.\n",
    "# 3) Third convolution layer works on the result of the preceding max pooling layer. \n",
    "#    Let's assume there are n/2 kernels in this layer. \n",
    "# Fully connected layers:\n",
    "# 1) First fully connected layer works on the result of the preceding max pooling layer. \n",
    "#    This layer is followed by hyperbolic tangent as its activation function.\n",
    "# 2) Second fully connected layer works on the result of the preceding activation function, and emits numbers associated\n",
    "#    with each class.\n",
    "# We will use negative log likelihood to compute the loss. So you may add additional layer(s) to your network.\n",
    "# Note: Since the network is parameterized (n), you'd rather define the CNN as a subclass of nn.Module.\n",
    "\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class Net(nn.Module):\n",
    "    def __init__(self, n = 32):\n",
    "        super().__init__()\n",
    "        self.n = n\n",
    "        \n",
    "        self.conv1 = nn.Conv2d(3, self.n, kernel_size=3, padding=1)\n",
    "        self.conv2 = nn.Conv2d(self.n, (self.n//2), kernel_size=3, padding=1)\n",
    "        self.conv3 = nn.Conv2d((self.n//2), (self.n//2), kernel_size=3, padding=1)\n",
    "\n",
    "        self.fc1 = nn.Linear((self.n//2)*4*4, 32)\n",
    "        self.fc2 = nn.Linear(32, 3)\n",
    "        \n",
    "        self.lsftmx = nn.LogSoftmax(dim=1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = F.max_pool2d(torch.tanh(self.conv1(x)), 2)\n",
    "        out = F.max_pool2d(torch.tanh(self.conv2(out)), 2)\n",
    "        out = F.max_pool2d(torch.tanh(self.conv3(out)), 2)\n",
    "\n",
    "        out = out.view(-1, (self.n//2)*4*4)\n",
    "        out = torch.tanh(self.fc1(out))\n",
    "        out = self.lsftmx(self.fc2(out))\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "1be13802-cee5-4150-b78c-49fae49d1c2a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(6419, 16163)"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Create two networks as instances of the CNN you defined above, with n = 16 and n = 32 respectively. \n",
    "# Print the total number of parameters in each of these instances.\n",
    "\n",
    "model_16 = Net(16)\n",
    "model_32 = Net(32)\n",
    "\n",
    "sum([param.numel() for param in model_16.parameters()]), sum([param.numel() for param in model_32.parameters()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "b7def331-3852-4cc7-9c97-52a1d5831523",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "469 94\n"
     ]
    }
   ],
   "source": [
    "# Our training functionality is supposed to compute gradient on batches of training data, randlomy selected each time.\n",
    "# To this end, create a training data loader with batch size 32 that randomizes access to each batch.\n",
    "# Also, create a validation data loader with the same batch size that does not randomize access to each batch (no need!)\n",
    "# Print the number of batches in training and validation data loaders\n",
    "\n",
    "train_loader = torch.utils.data.DataLoader(dataset_train_filtered, batch_size=32, shuffle=True)\n",
    "val_loader = torch.utils.data.DataLoader(dataset_val_filtered, batch_size=32, shuffle=False)\n",
    "\n",
    "print(len(train_loader), len(val_loader))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "a04ef16c-813a-459a-a65c-7d710321c97b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Define your training function that receives the training loader, model, loss function, optimizer, the device (cpu/gpu), and \n",
    "# number of epochs.\n",
    "#In each epoch, you should go through each training data batch, and:\n",
    "# 1) move data to device\n",
    "# 1) compute the output batch, and accordingly the loss\n",
    "# 2) compute the gradient of loss wrt parameters, and update the parameters\n",
    "#After covering all epochs, your training function must report the training accuracy\n",
    "\n",
    "def training_loop(n_epochs, model, loss_fn, optimizer, train_loader, device):\n",
    "    for epoch in range(1, n_epochs + 1):\n",
    "        for (imgs, labels) in train_loader:\n",
    "            imgs = imgs.to(device=device)\n",
    "            labels = labels.to(device=device)\n",
    "\n",
    "            outs = model(imgs)\n",
    "            loss = loss_fn(outs, labels)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "        if epoch % 10 == 0:\n",
    "            print(\"epoch=%d loss=%f\" %(epoch, loss)) # loss of last batch\n",
    "\n",
    "    total = 0\n",
    "    correct = 0\n",
    "    for imgs, labels in train_loader:\n",
    "        imgs = imgs.to(device=device)\n",
    "        labels = labels.to(device=device)\n",
    "\n",
    "        outs = model(imgs)\n",
    "        max_vals, max_classes = outs.max(dim=1)\n",
    "\n",
    "        total += imgs.shape[0]\n",
    "        correct += (max_classes == labels).sum()\n",
    "\n",
    "    print(\"training accuracy =%f\" %(correct/total))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "60dfa4c0-173f-4afd-87ee-e602276f330b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Define a separate function that receives the validation data loader as well as the model and computes the validation \n",
    "# accuracy of the model.\n",
    "\n",
    "def validate(model, loader, device):\n",
    "    total = 0\n",
    "    correct = 0\n",
    "    for (imgs, lbls) in loader:\n",
    "        imgs = imgs.to(device=device)\n",
    "        lbls = lbls.to(device=device)\n",
    "\n",
    "        outs = model(imgs)\n",
    "        max_vals, max_indexes = outs.max(dim=1)\n",
    "        \n",
    "        correct += ((max_indexes == lbls).sum())\n",
    "        total += imgs.shape[0]\n",
    "    print(\"accuracy %f\" %(correct/total))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "033e6166-15b7-4a07-aab5-eebbb2acc386",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch=10 loss=0.729366\n",
      "epoch=20 loss=0.644168\n",
      "epoch=30 loss=0.792817\n",
      "epoch=40 loss=0.568522\n",
      "epoch=50 loss=0.620974\n",
      "epoch=60 loss=0.183326\n",
      "epoch=70 loss=0.478958\n",
      "epoch=80 loss=0.174433\n",
      "epoch=90 loss=0.429307\n",
      "epoch=100 loss=0.208847\n",
      "training accuracy =0.890000\n",
      "accuracy 0.802667\n"
     ]
    }
   ],
   "source": [
    "#Define device dynamically based on whether CUDA is available or not.\n",
    "#Call the training function on the created training data loader, the created CNN  with n = 16, \n",
    "# negative log likelihood loss function, stochastic gradient descent optimizer,\n",
    "# the device you defined, and 100 epochs. Next, call validation accuracy function.\n",
    "#Is the model overfit? (Yes/No) Why?\n",
    "\n",
    "import torch.optim as optim\n",
    "\n",
    "device = torch.device(\"cuda\") if torch.cuda.is_available() else torch.device(\"cpu\")\n",
    "model_16 = Net(16).to(device=device)\n",
    "\n",
    "training_loop(\n",
    "    n_epochs=100,\n",
    "    model=model_16,\n",
    "    train_loader=train_loader,\n",
    "    loss_fn=nn.NLLLoss(),\n",
    "    optimizer=optim.SGD(model_16.parameters(), lr=0.01),\n",
    "    device=device\n",
    ")\n",
    "\n",
    "validate(model_16, val_loader, device=device)\n",
    "\n",
    "# Answer:\n",
    "#\n",
    "# No. Considering the obtained model accuracy data, we can conclude that the model is not overfitted.\n",
    "# Accuracy of 89% for training data and 80% for validation data indicates that the model does more than just memorize data and can handle unknown data. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "5d2f930c-039d-4123-b90d-60fac6d68afa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch=10 loss=0.706127\n",
      "epoch=20 loss=0.266369\n",
      "epoch=30 loss=0.505593\n",
      "epoch=40 loss=0.148179\n",
      "epoch=50 loss=0.235886\n",
      "epoch=60 loss=0.102419\n",
      "epoch=70 loss=0.057331\n",
      "epoch=80 loss=0.202786\n",
      "epoch=90 loss=0.146545\n",
      "epoch=100 loss=0.031223\n",
      "training accuracy =0.981067\n",
      "accuracy 0.808000\n"
     ]
    }
   ],
   "source": [
    "#Call the training function on the created training data loader, the created CNN  with n = 32, \n",
    "# negative log likelihood loss function, stochastic gradient descent optimizer,\n",
    "# the device you defined, and 100 epochs. Next, call validation accuracy function.\n",
    "#Is the model overfit? (Yes/No) Why? \n",
    "# (This can be compared to the fully connected network we created in the last set of exercises.)\n",
    "\n",
    "model_32 = Net(32).to(device=device)\n",
    "\n",
    "training_loop(\n",
    "    n_epochs=100,\n",
    "    model=model_32,\n",
    "    train_loader=train_loader,\n",
    "    loss_fn=nn.NLLLoss(),\n",
    "    optimizer=optim.SGD(model_32.parameters(), lr=0.01),\n",
    "    device=device\n",
    ")\n",
    "\n",
    "validate(model_32, val_loader, device=device)\n",
    "\n",
    "# Answer:\n",
    "#\n",
    "# Yes. Considering the obtained model accuracy data, we can assume that the model might be overfitted.\n",
    "# The big gap between accuracy of 98% for training data and 80% for validation data indicates \n",
    "# that the model performs worse on unknown data. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "1dde7111-bf9d-4a9b-b5df-f782586e60b0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch=10 loss=0.536367\n",
      "epoch=20 loss=0.753943\n",
      "epoch=30 loss=0.210761\n",
      "epoch=40 loss=0.345711\n",
      "epoch=50 loss=0.427637\n",
      "epoch=60 loss=0.227456\n",
      "epoch=70 loss=0.208162\n",
      "epoch=80 loss=0.400528\n",
      "epoch=90 loss=0.345810\n",
      "epoch=100 loss=0.071054\n",
      "training accuracy =0.932400\n",
      "accuracy 0.818333\n"
     ]
    }
   ],
   "source": [
    "#Next, let's consider L2 regularization with weight decay 0.002 for CNN with n = 32. \n",
    "# Is the model overfit? (Yes/No) Why?\n",
    "\n",
    "model_32 = Net(32).to(device=device)\n",
    "\n",
    "training_loop(\n",
    "    n_epochs=100,\n",
    "    model=model_32,\n",
    "    train_loader=train_loader,\n",
    "    loss_fn=nn.NLLLoss(),\n",
    "    optimizer=optim.SGD(model_32.parameters(), lr=0.01, weight_decay=0.004),\n",
    "    device=device\n",
    ")\n",
    "\n",
    "validate(model_32, val_loader, device=device)\n",
    "\n",
    "# Answer:\n",
    "#\n",
    "# No. Considering the obtained model accuracy data, we can conclude that the model is not overfitted.\n",
    "# The relatively small gap between accuracy of 93% for training data and 82% for validation data indicates\n",
    "# that the model does more than just memorize data and can handle unknown data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "6b8e315a-0f42-4dbf-b641-6c11fbe206e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Add a skip connection in your CNN from the output of second max pooling to the input of 3rd max pooling.\n",
    "#Train the updated CNN with the same parameters including (n = 32).\n",
    "#Is the model overfit? (Yes/No) Why?\n",
    "\n",
    "class NetSkip(nn.Module):\n",
    "    def __init__(self, n = 32):\n",
    "        super().__init__()\n",
    "        self.n = n\n",
    "        \n",
    "        self.conv1 = nn.Conv2d(3, self.n, kernel_size=3, padding=1)\n",
    "        self.conv2 = nn.Conv2d(self.n, (self.n//2), kernel_size=3, padding=1)\n",
    "        self.conv3 = nn.Conv2d((self.n//2), (self.n//2), kernel_size=3, padding=1)\n",
    "\n",
    "        self.fc1 = nn.Linear((self.n//2)*4*4, 32)\n",
    "        self.fc2 = nn.Linear(32, 3)\n",
    "        \n",
    "        self.lsftmx = nn.LogSoftmax(dim=1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = F.max_pool2d(torch.tanh(self.conv1(x)), 2)\n",
    "        out = F.max_pool2d(torch.tanh(self.conv2(out)), 2)\n",
    "\n",
    "        skip = out\n",
    "        out = F.max_pool2d(torch.tanh(self.conv3(out)) + skip, 2) # skip node\n",
    "\n",
    "        out = out.view(-1, (self.n//2)*4*4)\n",
    "        out = torch.tanh(self.fc1(out))\n",
    "        out = self.lsftmx(self.fc2(out))\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "cbe34828",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch=10 loss=0.341694\n",
      "epoch=20 loss=0.285952\n",
      "epoch=30 loss=0.430676\n",
      "epoch=40 loss=0.239788\n",
      "epoch=50 loss=0.262548\n",
      "epoch=60 loss=0.091029\n",
      "epoch=70 loss=0.049518\n",
      "epoch=80 loss=0.063178\n",
      "epoch=90 loss=0.035940\n",
      "epoch=100 loss=0.149305\n",
      "training accuracy =0.996467\n",
      "accuracy 0.812000\n"
     ]
    }
   ],
   "source": [
    "model_32 = NetSkip(32).to(device=device)\n",
    "\n",
    "training_loop(\n",
    "    n_epochs=100,\n",
    "    model=model_32,\n",
    "    train_loader=train_loader,\n",
    "    loss_fn=nn.NLLLoss(),\n",
    "    optimizer=optim.SGD(model_32.parameters(), lr=0.01),\n",
    "    device=device\n",
    ")\n",
    "\n",
    "validate(model_32, val_loader, device=device)\n",
    "\n",
    "# Answer:\n",
    "#\n",
    "# Yes. Considering the obtained model accuracy data, we can assume that the model might be overfitted.\n",
    "# The big gap between accuracy of 100% for training data and 81% for validation data indicates \n",
    "# that the model performs worse on unknown data. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "a4ebfaac-1a04-4695-aa15-c83f86290c36",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Consider dropout layers after each max pooling in the original CNN, where the probability of zeroing output features is 30%.\n",
    "#Train the updated CNN with the same parameters including (n = 32).\n",
    "#Is the model overfit? (Yes/No) Why?\n",
    "\n",
    "class NetDropout(nn.Module):\n",
    "    def __init__(self, n = 32):\n",
    "        super().__init__()\n",
    "        self.n = n\n",
    "        \n",
    "        self.conv1 = nn.Conv2d(3, self.n, kernel_size=3, padding=1)\n",
    "        self.conv1_dropout = nn.Dropout2d(p=0.3)\n",
    "        self.conv2 = nn.Conv2d(self.n, (self.n//2), kernel_size=3, padding=1)\n",
    "        self.conv2_dropout = nn.Dropout2d(p=0.3)\n",
    "        self.conv3 = nn.Conv2d((self.n//2), (self.n//2), kernel_size=3, padding=1)\n",
    "        self.conv3_dropout = nn.Dropout2d(p=0.3)\n",
    "\n",
    "        self.fc1 = nn.Linear((self.n//2)*4*4, 32)\n",
    "        self.fc2 = nn.Linear(32, 3)\n",
    "        \n",
    "        self.lsftmx = nn.LogSoftmax(dim=1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = F.max_pool2d(torch.tanh(self.conv1(x)), 2)\n",
    "        out = self.conv1_dropout(out)\n",
    "        out = F.max_pool2d(torch.tanh(self.conv2(out)), 2)\n",
    "        out = self.conv2_dropout(out)\n",
    "        out = F.max_pool2d(torch.tanh(self.conv3(out)), 2)\n",
    "        out = self.conv3_dropout(out)\n",
    "\n",
    "        out = out.view(-1, (self.n//2)*4*4)\n",
    "        out = torch.tanh(self.fc1(out))\n",
    "        out = self.lsftmx(self.fc2(out))\n",
    "        return out\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "f6f934bd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch=10 loss=0.883767\n",
      "epoch=20 loss=0.600886\n",
      "epoch=30 loss=0.845863\n",
      "epoch=40 loss=0.680702\n",
      "epoch=50 loss=0.601182\n",
      "epoch=60 loss=0.502269\n",
      "epoch=70 loss=0.528790\n",
      "epoch=80 loss=0.604622\n",
      "epoch=90 loss=1.034282\n",
      "epoch=100 loss=0.576950\n",
      "training accuracy =0.803600\n",
      "accuracy 0.781667\n"
     ]
    }
   ],
   "source": [
    "model_32 = NetDropout(32).to(device=device)\n",
    "\n",
    "training_loop(\n",
    "    n_epochs=100,\n",
    "    model=model_32,\n",
    "    train_loader=train_loader,\n",
    "    loss_fn=nn.NLLLoss(),\n",
    "    optimizer=optim.SGD(model_32.parameters(), lr=0.01),\n",
    "    device=device\n",
    ")\n",
    "\n",
    "validate(model_32, val_loader, device=device)\n",
    "\n",
    "# Answer:\n",
    "#\n",
    "# No. Considering the obtained model accuracy data, we can conclude that the model is not overfitted.\n",
    "# The small gap between accuracy of 80% for training data and 78% for validation data indicates\n",
    "# that the model does more than just memorize data and can handle unknown data. The model works the same with new data as with training data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "33917fa1-51d7-42cb-808d-7caa06ef8908",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Considering all the modifications which one works better? Plain CNN, CNN+L2, CNN+Skip, CNN+Dropout?\n",
    "\n",
    "# Answer:\n",
    "#\n",
    "# Model:        train_acc   val_acc\n",
    "# CNN:          98%         80%\n",
    "# CNN+L2:       93%         81%\n",
    "# CNN+Skip:     100%        81%\n",
    "# CNN+Dropout:  80%         78%\n",
    "#\n",
    "# All models show good accuracy results. An indicator of the effectiveness of working with unknown data is validation accuracy.\n",
    "# CNN+Dropout shows the lowest validation accuracy. However, it also has the smallest gap between training and validation accuracies.\n",
    "# This means that the model works the same with new data as with training data.\n",
    "# In my opinion, CNN+Dropout method is most suitable for preventing overfitting."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
